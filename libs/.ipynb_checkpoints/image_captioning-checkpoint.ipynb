{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74f5595",
   "metadata": {},
   "outputs": [],
   "source": [
    "Steps to train the model:\n",
    "* Take a pre-trained inception v3 to vectorize images\n",
    "* Stack an LSTM on top of it\n",
    "* Train on MSCOCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc168466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from random import choice\n",
    "from collections import defaultdict, Counter\n",
    "from textwrap import wrap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.transform import resize\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "from IPython.display import clear_output\n",
    "from models.beheaded_inception3 import beheaded_inception_v3\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d271ef56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset (vectorized images and captions)\n",
    "img_codes = np.load('data/image_codes.npy')\n",
    "captions = json.load(open('data/captions_tokenized.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f04ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split descriptions into tokens\n",
    "for img_i in range(len(captions)):\n",
    "    for caption_i in range(len(captions[img_i])):\n",
    "        sentence = captions[img_i][caption_i] \n",
    "        captions[img_i][caption_i] = [\"#START#\"] + sentence.split(' ') + [\"#END#\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652a6494",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# compute word frequencies for each word in captions\n",
    "word_counts = Counter()\n",
    "for img in captions:\n",
    "    for caption in img:\n",
    "        word_counts.update(caption)\n",
    "\n",
    "# build a vocabulary\n",
    "vocab  = ['#UNK#', '#START#', '#END#', '#PAD#']\n",
    "vocab += [k for k, v in word_counts.items() if v >= 5 if k not in vocab]\n",
    "n_tokens = len(vocab)\n",
    "\n",
    "word_to_index = {w: i for i, w in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5af259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for converting list of tokens into matrix\n",
    "\n",
    "eos_ix = word_to_index['#END#']\n",
    "unk_ix = word_to_index['#UNK#']\n",
    "pad_ix = word_to_index['#PAD#']\n",
    "\n",
    "def as_matrix(sequences, max_len=None):\n",
    "    \"\"\" Convert a list of tokens into a matrix with padding \"\"\"\n",
    "    max_len = max_len or max(map(len,sequences))\n",
    "    \n",
    "    matrix = np.zeros((len(sequences), max_len), dtype='int32') + pad_ix\n",
    "    for i,seq in enumerate(sequences):\n",
    "        row_ix = [word_to_index.get(word, unk_ix) for word in seq[:max_len]]\n",
    "        matrix[i, :len(row_ix)] = row_ix\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdb37a3",
   "metadata": {},
   "source": [
    "# Model \n",
    "\n",
    "<img src=\"https://github.com/yunjey/pytorch-tutorial/raw/master/tutorials/03-advanced/image_captioning/png/model.png\" style=\"width:70%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd7a0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_tokens, emb_size=128, lstm_units=256, cnn_feature_size=2048):\n",
    "        \"\"\" A recurrent 'head' network for image captioning. See scheme above. \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # a layer that converts conv features to initial_h (h_0) and initial_c (c_0)\n",
    "        self.cnn_to_h0 = nn.Linear(cnn_feature_size, lstm_units)\n",
    "        self.cnn_to_c0 = nn.Linear(cnn_feature_size, lstm_units)\n",
    "\n",
    "        # create embedding for input words. Use the parameters (e.g. emb_size).\n",
    "        self.embedding = nn.Embedding(n_tokens, emb_size)\n",
    "            \n",
    "        # lstm: create a recurrent core of your network. Use either LSTMCell or just LSTM. \n",
    "        # In the latter case (nn.LSTM), make sure batch_first=True\n",
    "        self.lstm = nn.LSTM(emb_size, lstm_units, batch_first=True)\n",
    "            \n",
    "        # create logits: linear layer that takes lstm hidden state as input and computes one number per token\n",
    "        self.rnn_to_logits = nn.Linear(lstm_units, n_tokens)\n",
    "        \n",
    "    def forward(self, image_vectors, captions_ix):\n",
    "        \"\"\" \n",
    "        Apply the network in training mode. \n",
    "        :param image_vectors: torch tensor containing inception vectors. shape: [batch, cnn_feature_size]\n",
    "        :param captions_ix: torch tensor containing captions as matrix. shape: [batch, word_i]. \n",
    "            padded with pad_ix\n",
    "        :returns: logits for next token at each tick, shape: [batch, word_i, n_tokens]\n",
    "        \"\"\"\n",
    "\n",
    "        self.lstm.flatten_parameters()\n",
    "\n",
    "        initial_cell = self.cnn_to_c0(image_vectors)\n",
    "        initial_hid = self.cnn_to_h0(image_vectors)\n",
    "        \n",
    "        # compute embeddings for captions_ix\n",
    "        emb_ix = self.embedding(captions_ix)\n",
    "        \n",
    "        # lstm_out should be lstm hidden state sequence of shape [batch, caption_length, lstm_units]\n",
    "        lstm_out, _ = self.lstm(emb_ix, (initial_cell[None], initial_hid[None]))\n",
    "        \n",
    "        # compute logits from lstm_out\n",
    "        logits = self.rnn_to_logits(lstm_out)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34245164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(network, image_vectors, captions_ix):\n",
    "    \"\"\"\n",
    "    :param image_vectors: torch tensor containing inception vectors. shape: [batch, cnn_feature_size]\n",
    "    :param captions_ix: torch tensor containing captions as matrix. shape: [batch, word_i]. \n",
    "        padded with pad_ix\n",
    "    :returns: crossentropy (neg llh) loss for next captions_ix given previous ones. Scalar float tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    # captions for input - all except last because we don't know next token for last one.\n",
    "    captions_ix_inp = captions_ix[:, :-1].contiguous()\n",
    "    captions_ix_next = captions_ix[:, 1:].contiguous()\n",
    "    \n",
    "    # apply the network, get predictions for captions_ix_next\n",
    "    logits_for_next = network.forward(image_vectors, captions_ix_inp)\n",
    "    \n",
    "    # compute the loss function between logits_for_next and captions_ix_next\n",
    "    loss = F.cross_entropy(\n",
    "        logits_for_next.permute((0,2,1)), \n",
    "        captions_ix_next, \n",
    "        ignore_index=pad_ix\n",
    "    )\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669400e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(img_codes, captions, batch_size, max_caption_len=None):\n",
    "    \n",
    "    # sample random numbers for image/caption indicies\n",
    "    random_image_ix = np.random.randint(0, len(img_codes), size=batch_size)\n",
    "    \n",
    "    # get images\n",
    "    batch_images = img_codes[random_image_ix]\n",
    "    \n",
    "    # captions for each image\n",
    "    captions_for_batch_images = captions[random_image_ix]\n",
    "    \n",
    "    # pick one from a set of captions for each image\n",
    "    batch_captions = list(map(choice,captions_for_batch_images))\n",
    "    \n",
    "    # convert to matrix\n",
    "    batch_captions_ix = as_matrix(batch_captions,max_len=max_caption_len)\n",
    "    \n",
    "    return torch.tensor(batch_images, dtype=torch.float32), \\\n",
    "        torch.tensor(batch_captions_ix, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51ed9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(network, vectorizer, image, caption_prefix = ('#START#',), t=1, sample=True, max_len=100):\n",
    "    network = network.cpu().eval()\n",
    "\n",
    "    assert isinstance(image, np.ndarray) and np.max(image) <= 1\\\n",
    "           and np.min(image) >= 0 and image.shape[-1] == 3\n",
    "    \n",
    "    image = torch.tensor(image.transpose([2, 0, 1]), dtype=torch.float32)\n",
    "    \n",
    "    vectors_8x8, vectors_neck, logits = vectorizer(image[None])\n",
    "    caption_prefix = list(caption_prefix)\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        \n",
    "        prefix_ix = as_matrix([caption_prefix])\n",
    "        prefix_ix = torch.tensor(prefix_ix, dtype=torch.int64)\n",
    "        next_word_logits = network.forward(vectors_neck, prefix_ix)[0, -1]\n",
    "        next_word_probs = F.softmax(next_word_logits, -1).detach().numpy()\n",
    "        \n",
    "        assert len(next_word_probs.shape) == 1, 'probs must be one-dimensional'\n",
    "        next_word_probs = next_word_probs ** t / np.sum(next_word_probs ** t) # apply temperature\n",
    "\n",
    "        if sample:\n",
    "            next_word = np.random.choice(vocab, p=next_word_probs) \n",
    "        else:\n",
    "            next_word = vocab[np.argmax(next_word_probs)]\n",
    "\n",
    "        caption_prefix.append(next_word)\n",
    "\n",
    "        if next_word == '#END#':\n",
    "            break\n",
    "\n",
    "    return ' '.join(caption_prefix[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c390bcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset\n",
    "captions = np.array(captions)\n",
    "train_img_codes, val_img_codes, train_captions, val_captions = train_test_split(\n",
    "    img_codes, captions, test_size=0.1, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf61dc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    network,\n",
    "    optimizer,\n",
    "    checkpoint_in, \n",
    "    train_img_codes,\n",
    "    train_captions,\n",
    "    val_img_codes,\n",
    "    val_captions,\n",
    "    batch_size=128,\n",
    "    n_epochs_to_train=100,\n",
    "    n_batches_per_epoch=50,\n",
    "    n_validation_batches=5,   \n",
    "    max_epochs_to_improve=5\n",
    "    device: torch.device = torch.device('cpu'),\n",
    "):\n",
    "\n",
    "    '''\n",
    "    function to performe:\n",
    "    - training of the network (network)\n",
    "        using optimizer (optimizer)\n",
    "        on the training set of images (train_img_codes) and \n",
    "        captions (train_captions)\n",
    "    - validation of the results \n",
    "        on the validating set of images (val_img_codes) and \n",
    "        captions (val_captions)\n",
    "    function keeps track of the training and validation losses\n",
    "    function starts training from the specified checkpoint (checkpoint_in)\n",
    "    and saves and returns the checkpoint after specified number of training epochs (n_epochs_to_train)\n",
    "    is perfomed\n",
    "    function stops training if the following early stopping criterion is satisfied:\n",
    "    - if after reaching local minimum validation loss exceeds the local minimum \n",
    "    in more than (max_epochs_to_improve) consecutive epochs, then training stops   \n",
    "    '''\n",
    "\n",
    "    # initialize dictionary to track \n",
    "    # training and validation losses\n",
    "    if not checkpoint_in:\n",
    "        checkpoint_in = {\n",
    "            'epoch': 1,\n",
    "            'state_dict': network_n.state_dict(),\n",
    "            'optimizer': optimizer_n.state_dict(),\n",
    "            'train_loss': [],\n",
    "            'valid_loss': []\n",
    "        }    \n",
    "    \n",
    "    HISTORY = collections.defaultdict(list)\n",
    "    HISTORY['train_loss'] = checkpoint_in['train_loss'].copy()\n",
    "    HISTORY['valid_loss'] = checkpoint_in['valid_loss'].copy()\n",
    "\n",
    "    network.to(device)\n",
    "\n",
    "    # load initial state of the network  \n",
    "    network.load_state_dict(checkpoint_in['state_dict'])        \n",
    "    # load initial state of the optimizer\n",
    "    optimizer.load_state_dict(checkpoint_in['optimizer'])\n",
    "    # get the number of the initial epoch\n",
    "    start_epoch = checkpoint_in['epoch']\n",
    "\n",
    "    # set the initial local minimum of the validation loss\n",
    "    if len(checkpoint_in['valid_loss']) == 0:\n",
    "        min_valid_loss = np.inf\n",
    "    else:    \n",
    "        min_valid_loss = min(checkpoint_in['valid_loss'])\n",
    "\n",
    "    # set the initial number of validation loss values\n",
    "    # exceeding local minimum \n",
    "    epochs_to_improve = 0\n",
    "\n",
    "    # for each training epoch \n",
    "    for epoch in range(1, n_epochs_to_train + 1):    \n",
    "        # train the network\n",
    "        # and compute the training loss of the epoch\n",
    "        train_loss = 0\n",
    "        network.train()\n",
    "        for _ in tqdm(range(n_batches_per_epoch)):\n",
    "            images, captions = generate_batch(train_img_codes, train_captions, batch_size)\n",
    "            images = images.to(DEVICE)\n",
    "            captions = captions.to(DEVICE)\n",
    "            loss_t = compute_loss(network, images, captions)\n",
    "            # clear old gradients\n",
    "            optimizer.zero_grad()\n",
    "            # do a backward pass\n",
    "            loss_t.backward()\n",
    "            # next step\n",
    "            optimizer.step()\n",
    "            train_loss += loss_t.detach().cpu().numpy()\n",
    "\n",
    "        # calculate and store the training loss of the epoch    \n",
    "        train_loss /= n_batches_per_epoch\n",
    "        HISTORY['train_loss'].append(train_loss[0])\n",
    "\n",
    "        # and compute the validating loss of the epoch\n",
    "        val_loss = 0\n",
    "        network.eval()\n",
    "        for _ in range(n_validation_batches):\n",
    "            images, captions = generate_batch(val_img_codes, val_captions, batch_size)\n",
    "            images = images.to(DEVICE)\n",
    "            captions = captions.to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                loss_t = compute_loss(network, images, captions)\n",
    "            val_loss += loss_t.detach().cpu().numpy()\n",
    "\n",
    "        # calculate and store the validating loss of the epoch\n",
    "        val_loss /= n_validation_batches\n",
    "        HISTORY['valid_loss'].append(val_loss[0])\n",
    "\n",
    "        # visualize train and validation losses\n",
    "        display.clear_output()       \n",
    "        print(f'\\nEpoch: {start_epoch + epoch}, train loss: {train_loss[0]:.4f}, validation loss: {val_loss[0]:.4f}')\n",
    "\n",
    "        # plot training and validation losses     \n",
    "        fig, axes = plt.subplots(1, 1, figsize=(7, 7))    \n",
    "        axes.set_title('Loss (Cross Entropy)')\n",
    "        axes.plot(range(1, start_epoch + epoch + 1), HISTORY['train_loss'], label='Train Loss')\n",
    "        axes.plot(range(1, start_epoch + epoch + 1), HISTORY['valid_loss'], label='Validation Loss')\n",
    "        axes.set_xticks(range(1, start_epoch + epoch + 1))\n",
    "        axes.grid()\n",
    "        axes.legend(fontsize=20)    \n",
    "        plt.show()\n",
    "\n",
    "        # increase counter of validation loss values\n",
    "        # exceeding local minimum   \n",
    "        if val_loss[0] > min_valid_loss:\n",
    "            epochs_to_improve += 1\n",
    "        # update local minimum for validation loss\n",
    "        # and zero the counter  \n",
    "        elif val_loss[0] < min_valid_loss:\n",
    "            min_valid_loss = val_loss[0]\n",
    "            epochs_to_improve = 0\n",
    "\n",
    "        # apply early stopping criterion\n",
    "        if epochs_to_improve > max_epochs_to_improve:\n",
    "            print('\\nEarly stopping criteria satisfied!')\n",
    "            break     \n",
    "\n",
    "    # save checkpoint after training\n",
    "    checkpoint_out = {\n",
    "    'epoch': start_epoch + epoch,\n",
    "    'state_dict': network.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'train_loss': HISTORY['train_loss'],\n",
    "    'valid_loss': HISTORY['valid_loss']\n",
    "    }\n",
    "\n",
    "    print(f\"\\nTraining completed after {start_epoch + epoch} epochs!\")\n",
    "\n",
    "    return checkpoint_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9d2cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(checkpoint):\n",
    "  '''\n",
    "  function to print last observed and average loss metrics\n",
    "  on train and validation datasets for the checkpoint provided\n",
    "  '''\n",
    "\n",
    "  print(f\"\\nAfter training the network for {checkpoint['epoch']-1} epoches we achieved: \\\n",
    "        \\n last train loss: {checkpoint['train_loss'][-1]:0.4f} \\\n",
    "        \\n average train loss: {np.mean(checkpoint['train_loss']):0.4f} \\\n",
    "        \\n last validation loss: {checkpoint['valid_loss'][-1]:0.4f} \\\n",
    "        \\n average validation loss: {np.mean(checkpoint['valid_loss']):0.4f} \\\n",
    "        \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f3a346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to the checkpoint file\n",
    "MODEL_CHECKPOINT_PATH = 'checkpoint.pt'\n",
    "# set number of training epochs\n",
    "NUM_OF_TRAINING_EPOCHS = 30\n",
    "# set the batch size\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5626a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# create the model\n",
    "model = CaptionNet(n_tokens)\n",
    "# create an optimizer\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
    "# try to load checkpoint\n",
    "checkpoint_in = None\n",
    "try:\n",
    "    checkpoint_in = torch.load(MODEL_CHECKPOINT_PATH, map_location=device)\n",
    "except FileNotFoundError:\n",
    "    print('unable to load model checkpoint\\n')\n",
    "\n",
    "# train the network for 6 epoch\n",
    "checkpoint_out = train(\n",
    "    model,\n",
    "    optimizer,\n",
    "    checkpoint_in, \n",
    "    train_img_codes, \n",
    "    train_captions,\n",
    "    val_img_codes,\n",
    "    val_captions,\n",
    "    batch_size=BATCH_SIZE, \n",
    "    n_epochs_to_train=n_epochs_to_train,    \n",
    "    n_batches_per_epoch=n_batches_per_epoch, \n",
    "    n_validation_batches=n_validation_batches\n",
    "    )\n",
    "\n",
    "# print metrics after training\n",
    "print_metrics(checkpoint_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
